{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tYmiOqfQMha7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbaa3eb-2140-430e-bd09-666d9b17c7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "cUBWxZ5QMpM6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the libraries, and the ImageDataGenerator for CNN"
      ],
      "metadata": {
        "id": "HR1mpzCOIQkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "UtBVB4EgM3PH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "127ba2bc-c953-4bee-90e3-f0ec1a2b1e33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.14.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/CNN/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "metadata": {
        "id": "LEpINWGxHvnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d24f83-17af-49c5-95b1-8d1e46934ffc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20030 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is a data preprocessing of the training set. ImageDataGenerator first recales the images then shear range, zoom range, and horzontal flip mentioned more in detail in the API. this also considered a feature scaling, then transform thje images after this we connect it to the training set with flow from directory method. binary is used for the images."
      ],
      "metadata": {
        "id": "2mVO2T78IXzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/CNN/test',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "metadata": {
        "id": "wlRsWpcWKI1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf24009-e35c-4382-b7e0-29c73f20aae0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing the testing set, first we feature scale and we dont need any traformations since there might be a leakage, and connect to the test set. target and batch size are the same as above."
      ],
      "metadata": {
        "id": "_7JplfrFInRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "uGlefwccKU8h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "initizallzing the CNN using sequencial"
      ],
      "metadata": {
        "id": "4JJxDLtZKLv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "GWhiIsftLDZK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining the convolution with the conv2D filters are diferent feature detectors, kernel size is a 3x3, using relu as an activtion function, input shape is just the color."
      ],
      "metadata": {
        "id": "IXmABn1NKQE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "metadata": {
        "id": "uWWY_ANkKbFw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we add a seocnd convolutional layer, we dont need an input shape since we added it in the first"
      ],
      "metadata": {
        "id": "C7aiUWntK0z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "0h7509fcKtcY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "DhLEn89pLLvn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "flatenning"
      ],
      "metadata": {
        "id": "Hfo_IXkMMBmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "metadata": {
        "id": "W42Ua0heLL9f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "zrkkhbRiLcOg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer =\"adam\" , loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "5E-uWuWeLnHZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here CNN is being compiled"
      ],
      "metadata": {
        "id": "Gwedz4NDMNeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "metadata": {
        "id": "bRQJmDyGLtrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6bbb61-b526-41e4-f225-91139ddca5a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "626/626 [==============================] - 3281s 5s/step - loss: 0.6455 - accuracy: 0.6220 - val_loss: 0.5888 - val_accuracy: 0.6932\n",
            "Epoch 2/25\n",
            "626/626 [==============================] - 165s 264ms/step - loss: 0.5676 - accuracy: 0.7055 - val_loss: 0.5655 - val_accuracy: 0.7092\n",
            "Epoch 3/25\n",
            "626/626 [==============================] - 165s 264ms/step - loss: 0.5247 - accuracy: 0.7372 - val_loss: 0.4976 - val_accuracy: 0.7706\n",
            "Epoch 4/25\n",
            "626/626 [==============================] - 166s 265ms/step - loss: 0.4958 - accuracy: 0.7545 - val_loss: 0.4810 - val_accuracy: 0.7694\n",
            "Epoch 5/25\n",
            "626/626 [==============================] - 163s 260ms/step - loss: 0.4751 - accuracy: 0.7728 - val_loss: 0.4835 - val_accuracy: 0.7762\n",
            "Epoch 6/25\n",
            "626/626 [==============================] - 163s 260ms/step - loss: 0.4554 - accuracy: 0.7863 - val_loss: 0.4584 - val_accuracy: 0.7856\n",
            "Epoch 7/25\n",
            "626/626 [==============================] - 162s 258ms/step - loss: 0.4404 - accuracy: 0.7935 - val_loss: 0.4625 - val_accuracy: 0.7830\n",
            "Epoch 8/25\n",
            "626/626 [==============================] - 160s 256ms/step - loss: 0.4238 - accuracy: 0.8034 - val_loss: 0.4505 - val_accuracy: 0.7902\n",
            "Epoch 9/25\n",
            "626/626 [==============================] - 168s 268ms/step - loss: 0.4142 - accuracy: 0.8077 - val_loss: 0.4642 - val_accuracy: 0.7868\n",
            "Epoch 10/25\n",
            "626/626 [==============================] - 165s 263ms/step - loss: 0.4026 - accuracy: 0.8152 - val_loss: 0.4476 - val_accuracy: 0.8014\n",
            "Epoch 11/25\n",
            "626/626 [==============================] - 163s 260ms/step - loss: 0.3900 - accuracy: 0.8231 - val_loss: 0.4550 - val_accuracy: 0.7956\n",
            "Epoch 12/25\n",
            "626/626 [==============================] - 167s 267ms/step - loss: 0.3774 - accuracy: 0.8294 - val_loss: 0.4247 - val_accuracy: 0.8038\n",
            "Epoch 13/25\n",
            "626/626 [==============================] - 163s 261ms/step - loss: 0.3561 - accuracy: 0.8438 - val_loss: 0.4470 - val_accuracy: 0.8028\n",
            "Epoch 14/25\n",
            "626/626 [==============================] - 166s 266ms/step - loss: 0.3504 - accuracy: 0.8461 - val_loss: 0.5303 - val_accuracy: 0.7704\n",
            "Epoch 15/25\n",
            "626/626 [==============================] - 163s 260ms/step - loss: 0.3411 - accuracy: 0.8499 - val_loss: 0.4843 - val_accuracy: 0.7892\n",
            "Epoch 16/25\n",
            "626/626 [==============================] - 163s 261ms/step - loss: 0.3227 - accuracy: 0.8582 - val_loss: 0.4461 - val_accuracy: 0.7964\n",
            "Epoch 17/25\n",
            "626/626 [==============================] - 166s 265ms/step - loss: 0.3175 - accuracy: 0.8635 - val_loss: 0.4459 - val_accuracy: 0.8070\n",
            "Epoch 18/25\n",
            "626/626 [==============================] - 163s 260ms/step - loss: 0.3082 - accuracy: 0.8685 - val_loss: 0.4487 - val_accuracy: 0.7970\n",
            "Epoch 19/25\n",
            "626/626 [==============================] - 161s 257ms/step - loss: 0.2939 - accuracy: 0.8750 - val_loss: 0.4528 - val_accuracy: 0.8014\n",
            "Epoch 20/25\n",
            "626/626 [==============================] - 162s 259ms/step - loss: 0.2826 - accuracy: 0.8791 - val_loss: 0.4593 - val_accuracy: 0.8054\n",
            "Epoch 21/25\n",
            "626/626 [==============================] - 164s 262ms/step - loss: 0.2814 - accuracy: 0.8797 - val_loss: 0.4668 - val_accuracy: 0.8080\n",
            "Epoch 22/25\n",
            "626/626 [==============================] - 165s 263ms/step - loss: 0.2634 - accuracy: 0.8851 - val_loss: 0.4622 - val_accuracy: 0.8014\n",
            "Epoch 23/25\n",
            "626/626 [==============================] - 162s 258ms/step - loss: 0.2565 - accuracy: 0.8916 - val_loss: 0.5285 - val_accuracy: 0.7910\n",
            "Epoch 24/25\n",
            "626/626 [==============================] - 163s 261ms/step - loss: 0.2476 - accuracy: 0.8974 - val_loss: 0.4986 - val_accuracy: 0.8008\n",
            "Epoch 25/25\n",
            "626/626 [==============================] - 164s 262ms/step - loss: 0.2433 - accuracy: 0.9028 - val_loss: 0.4983 - val_accuracy: 0.8040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b2003c3e9e0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i use the fit method to train the model. we change the epoch in order to see the convolutional value."
      ],
      "metadata": {
        "id": "6oDDEN-aMYZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/My Drive/Colab Notebooks/CNN/test/dogs/dog.12491.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "metadata": {
        "id": "4l6sCJP-L32A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc634e7-dccb-477d-b1ce-2804aa7930e5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 133ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we make a single prediction using the load _img function, caluyes need to bes ame as before in the train and test which is 64,64 then we use ghe image toarray in order to accpeth the predict method. we also add an extra dimension using the first dimenssion. predcit method will predict and result[0,0] batch and elemnt 0"
      ],
      "metadata": {
        "id": "1cjGPObvMvkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "id": "iVIL7saMPNeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db45ea77-1fa2-4c07-8e73-c8579169ee07"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ]
    }
  ]
}